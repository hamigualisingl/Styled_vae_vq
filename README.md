# Styled_vae_vq
<div align="center">
Author: lidehu
</div>

## åŠ¨æœºï¼š 
åº”å¯¹å›¾åƒç†è§£ä¸è‡ªå›å½’ç”Ÿæˆéš¾é¢˜ï¼Œæ¯”å¦‚:ç†è§£-æ³¨æ„åŠ›åˆ†æ•£ï¼Œå‡ºç°å¹»è§‰ï¼›ç”Ÿæˆ-æ•°æ®ä¸å¯¹é½ï¼Œä¸‹ä¸€ä¸ªtokené€‰æ‹©è¿‡å¤šï¼Œè‡ªå›å½’ç”Ÿæˆå›°éš¾ï¼Œæå‡ºStyled_vae_vqï¼Œå°†å›¾åƒæŒ‰åºç¦»æ•£æˆ36ä¸ªä»é«˜çº§åˆ°ä½çº§çš„å±æ€§tokenã€‚ 


## æ¨¡å‹ç»“æ„
- ç¼–ç å™¨ï¼šä¸€ä¸ªè¾“å…¥å·ç§¯å±‚input_covï¼Œ256ä¸ªä½ç½®tokenå’Œ36ä¸ªç‰¹æ®Štokenï¼Œ24å±‚Transformer,æœ«å°¾ä¸ºä¿©ä¸ªmean,stdé¢„æµ‹å¤´ã€‚
- è§£ç å™¨ï¼š256ä¸ªä½ç½®tokenå’Œ36ä¸ªå ä½tokenï¼Œ18ä¸ªcondtionTransformerå±‚ï¼Œ6å±‚Transformerï¼Œè¾“å‡ºå·ç§¯å±‚out_covã€‚
## æ•°æ®æµåŠ¨æµç¨‹ï¼š
- ç¼–ç å™¨ï¼šimg(bs,3,256,256)->input_cov(img)->(256,bs,1024)->add(256ä¸ªä½ç½®token)
->cat(x,36ä¸ªç‰¹æ®Štoken)->Transformer->å–å‡ºå36ä¸ªtokenä½œä¸ºcondtion
- è§£ç å™¨ï¼š256ä¸ªä½ç½®token->condtionTransformer->Transformer->å–å256ä¸ªtoken->out_cov->é‡å»ºæŸå¤±
- æ¡ä»¶æ·»åŠ æ–¹å¼å¦‚ä¸‹:
    ```
    for index, r in enumerate(self.condtionTransformer):
        x = r(x,conditon[index*2:(index+1)*2], index)
    ```
    ```
    zeros_like_x = torch.zeros_like(x)(36+256,bs,dim)
    zeros_like_x[index*2:(index+1)*2]=zeros_like_x[index*2:(index+1)*2]+ condation
    x=x+ zeros_like_x
    norm_hidden_states = self.norm1(x,self.ln_1(self.condation_1(x[index*2])+condation[0]))
    x = x + self.attn1(norm_hidden_states, norm_hidden_states, norm_hidden_states, need_weights=False, attn_mask=attn_mask)[0]#([516, 516]) torch.float32
    x = x + self.mlp(self.norm2(x,self.ln_2(self.condation_2(x[index*2+1])+condation[1])))
       
    ```
- æ¡ä»¶æ·»åŠ æ–¹æ¡ˆè§£é‡Šï¼šæ•´ä½“æ·»åŠ æ–¹å¼ç±»ä¼¼äºstylegan2çš„w+ï¼Œç›¸å…³stylegan2W+çš„åˆ†æä¸å†èµ˜è¿°ã€‚ä¸ºäº†è¿›ä¸€æ­¥åŠ å¼ºç”Ÿæˆåºåˆ—çš„çº ç¼ å…³è”æœ‰é€»è¾‘ï¼Œå’Œä¿ƒä½¿36ä¸ªtokenï¼Œæ˜¯ä»é«˜åˆ°ä½çš„å±æ€§ç»„åˆï¼Œä½¿ç”¨äº†å ä½tokenï¼Œå½“å‰æ¡ä»¶token2éœ€è¦ä¸å ä½tokenç›¸åŠ ï¼Œè€Œå ä½tokenåˆ°è¾¾æœ¬å±‚æ—¶å€™ï¼Œå·²ç»è¢«å‰é¢çš„æ¡ä»¶tokenæ”¹å˜ï¼Œå½±å“ã€‚å› æ­¤å¦‚æœ36ä¸ªtokenï¼Œæ˜¯ä»ä½åˆ°é«˜çš„å±æ€§ç»„åˆï¼Œæˆ–è€…æ²¡æœ‰ç›¸å…³æ€§ï¼Œè§£ç å™¨å¤åŸå›¾ç‰‡å˜å¾—å¾ˆå›°éš¾ï¼Œæµ“æµ“çš„çœ‰æ¯›å±æ€§å½±å“åˆ°æ€§åˆ«æ˜¯ç”·æ€§å±æ€§ï¼Œè¿™ä¸ªä¼šç»™å¤åŸå¸¦æ¥å¾ˆå¤§éš¾åº¦ï¼Œä½†å¦‚æœå…ˆæŒ‡æ˜è¿™ä¸ªäººæ˜¯ç”·æ€§å±æ€§ç„¶åå½±å“çœ‰æ¯›å±æ€§ï¼Œè¿™å°±åˆç†äº›ã€‚
## Instructions
- ### Environment installation

    ```
    pip install -r requirments.txt
    ```
- ### Dataset preparation
    
    1ã€Download YFCC15M

    The YFCC15M dataset we used is [YFCC15M-DeCLIP](https://arxiv.org/abs/2110.05208), we download it from the [repo](https://github.com/AdamRain/YFCC15M_downloader), finally we successful donwload 15061515 image-text pairs.

    2ã€Generate synthetic caption

    In our paper, we use OFA model to generate synthetic captions. You can download model weight and scripts from the [OFA](https://github.com/OFA-Sys/OFA) project.

    3ã€Generate rec files

    To improve the training efficience, we use [MXNet](https://github.com/apache/mxnet) to save the YFCC15M dataset to rec file, and use NVIDIA [DALI](https://github.com/NVIDIA/DALI) to accelerate data loading and pre-processing. The sample code to generate rec files is in [data2rec.py](data2rec.py).

- ### Pretrained Model Weight

    You can download the pretrained model weight from [Google Drive](https://drive.google.com/file/d/1AqSHisCKZOZ16Q3sYguK6zIZIuwwEriE/view?usp=share_link) or [BaiduYun](https://pan.baidu.com/s/10dFfvGMWeaTXUyrZlZlCEw?pwd=xftg), and you can find the traning log in [Google Drive](https://drive.google.com/file/d/1I8gdSQCJAfFamDcVztwW8EQIc_OOK8Xh/view?usp=share_link) or [BaiduYun](https://pan.baidu.com/s/1oz0UVzX2N0Sri7MfwR-kog?pwd=7ki7)

- ### Training

    Start training by run
    ```
    bash scripts/train_yfcc15m_B32_ALIP.sh
    ```

- ### Evaluation

    Evaluate zero shot cross-modal retireval

    ```
    bash run_retrieval.sh
    ```
    Evaluate zero shot classification

    ```
    bash run_zeroshot.sh
    ```



## Acknowledgement

This project is based on [open_clip](https://github.com/mlfoundations/open_clip) and [OFA](https://github.com/OFA-Sys/OFA), thanks for their works.

## License

This project is released under the MIT license. Please see the [LICENSE](LICENSE) file for more information.

## Citation
If you find this repository useful, please use the following BibTeX entry for citation.

```latex
@misc{yang2023alip,
      title={ALIP: Adaptive Language-Image Pre-training with Synthetic Caption}, 
      author={Kaicheng Yang and Jiankang Deng and Xiang An and Jiawei Li and Ziyong Feng and Jia Guo and Jing Yang and Tongliang Liu},
      year={2023},
      eprint={2308.08428},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
```

## ğŸŒŸStar History

[![Star History Chart](https://api.star-history.com/svg?repos=deepglint/ALIP&type=Date)](https://star-history.com/#deepglint/ALIP&Date)

